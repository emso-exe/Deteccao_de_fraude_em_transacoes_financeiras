{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construção da ABT\n",
    "_ABT - Analytical Base Table_\n",
    "\n",
    "---\n",
    "\n",
    "## Sumário\n",
    "\n",
    "1. **Importação da bibliotecas**\n",
    "2. **Criação e iniciação de uma sessão PySpark**\n",
    "3. **Criação dos datasets a partir da leitura dos arquivos *.csv e *.json**\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, expr\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Criação e iniciação de uma sessão PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://rootx:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ABT - Transações de clientes</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2ab20235be0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appName = 'ABT - Transações de clientes'\n",
    "\n",
    "# O objeto SparkSession configurado com configurações específicas\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(appName) \\\n",
    "    .config('spark.driver.memory', '8g') \\\n",
    "    .config('spark.executor.memory', '8g') \\\n",
    "    .config('spark.master', 'local[*]') \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Criação dos datasets a partir da leitura dos arquivos *.csv e *.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho dos arquivos *.csv e *.json\n",
    "caminho = 'dados/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- client_id: integer (nullable = true)\n",
      " |-- card_brand: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- card_number: long (nullable = true)\n",
      " |-- expires: string (nullable = true)\n",
      " |-- cvv: integer (nullable = true)\n",
      " |-- has_chip: string (nullable = true)\n",
      " |-- num_cards_issued: integer (nullable = true)\n",
      " |-- credit_limit: string (nullable = true)\n",
      " |-- acct_open_date: string (nullable = true)\n",
      " |-- year_pin_last_changed: integer (nullable = true)\n",
      " |-- card_on_dark_web: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dados de cartão de crédito\n",
    "df_cards = spark.read.csv(caminho + 'cards_data.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Exibir o esquema do dataframe\n",
    "df_cards.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- client_id: integer (nullable = true)\n",
      " |-- card_id: integer (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- use_chip: string (nullable = true)\n",
      " |-- merchant_id: integer (nullable = true)\n",
      " |-- merchant_city: string (nullable = true)\n",
      " |-- merchant_state: string (nullable = true)\n",
      " |-- zip: double (nullable = true)\n",
      " |-- mcc: integer (nullable = true)\n",
      " |-- errors: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dados de transações\n",
    "df_transactions = spark.read.csv(caminho + 'transactions_data.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Exibir o esquema do dataframe\n",
    "df_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- current_age: integer (nullable = true)\n",
      " |-- retirement_age: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- birth_month: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- per_capita_income: string (nullable = true)\n",
      " |-- yearly_income: string (nullable = true)\n",
      " |-- total_debt: string (nullable = true)\n",
      " |-- credit_score: integer (nullable = true)\n",
      " |-- num_credit_cards: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dados de clientes\n",
    "df_clients = spark.read.csv(caminho + 'users_data.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Exibir o esquema do dataframe\n",
    "df_clients.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_to_csv(json_path, csv_path, list_columns, one_column=None):\n",
    "    '''\n",
    "    Processa um arquivo JSON e converte em CSV.\n",
    "\n",
    "    :param json_path: path + filename.json\n",
    "        Caminho do arquivo JSON de entrada.\n",
    "    :param csv_path: path + filename.csv\n",
    "        Caminho do arquivo CSV de saída.\n",
    "    :param list_columns: list\n",
    "        Lista com os nomes das colunas do CSV.\n",
    "    :param one_column: string (default=None)\n",
    "        Nome da chave no JSON para acessar dados aninhados (opcional).\n",
    "    '''\n",
    "    # Carregar o arquivo JSON\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        \n",
    "    if not data:\n",
    "        print('Arquivo JSON está vazio.')\n",
    "        return\n",
    "    \n",
    "    # Processar dados aninhados, se necessário\n",
    "    if one_column:\n",
    "        data = data.get(one_column, {})\n",
    "        if not data:\n",
    "            print(f'A chave \\'{one_column}\\' não foi encontrada ou está vazia no JSON.')\n",
    "            return\n",
    "        print(f'Dados extraídos da chave \\'{one_column}\\'.')\n",
    "    else:\n",
    "        print('Processando JSON sem dados aninhados.')\n",
    "\n",
    "    # Escrever o CSV\n",
    "    with open(csv_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(list_columns)  # Escrevendo o cabeçalho\n",
    "        \n",
    "        for key, value in data.items():\n",
    "            writer.writerow([key, value])  # Escrevendo as linhas\n",
    "    \n",
    "    print(f'Arquivo CSV gerado com sucesso em {csv_path}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando JSON sem dados aninhados.\n",
      "Arquivo CSV gerado com sucesso em dados/mcc_codes.csv.\n"
     ]
    }
   ],
   "source": [
    "# Processar mcc_codes.json\n",
    "json_path = caminho + 'mcc_codes.json'\n",
    "csv_path = caminho + 'mcc_codes.csv'\n",
    "list_columns = ['code', 'description']\n",
    "process_json_to_csv(json_path, csv_path, list_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados extraídos da chave 'target'.\n",
      "Arquivo CSV gerado com sucesso em dados/train_fraud_labels.csv.\n"
     ]
    }
   ],
   "source": [
    "# Processar train_fraud_labels.json\n",
    "json_path = caminho + 'train_fraud_labels.json'\n",
    "csv_path = caminho + 'train_fraud_labels.csv'\n",
    "list_columns = ['transaction_id', 'is_fraud']\n",
    "one_column = 'target'\n",
    "process_json_to_csv(json_path, csv_path, list_columns, one_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
